# -*- coding: utf-8 -*-
"""122/2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iFQGtI2Wd7Om7R7yUAdhZGI5VLAV3uVE

---



# ***`Predicting the sales of products across stores of a retail chain`***



---

A large Indian retail chain has stores across 3 states in India: Maharashtra, Telangana and 
Kerala. These stores stock products across various categories such as FMCG (fast moving 
consumer goods), eatables / perishables and others. Managing the inventory is crucial for 
the revenue stream of the retail chain. Meeting the demand is important to not lose 
potential revenue, while at the same time stocking excessive products could lead to losses.

---
"""

# Commented out IPython magic to ensure Python compatibility.
#import Neccessory libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


from sklearn.preprocessing import StandardScaler


from sklearn.model_selection import train_test_split, GridSearchCV

#import required accuracy metrics
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.metrics import roc_curve, roc_auc_score
from sklearn.model_selection import KFold, cross_val_score

import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline

"""

---



---

"""

week_id = pd.read_csv('/content/date_to_week_id_map.csv')
week_id.head(2)

product_price = pd.read_csv('/content/product_prices.csv')
product_price.head(3)

train = pd.read_csv('/content/train_data.csv')
train.head(3)

tdf = pd.merge(product_price,week_id, on=['week_id'], how='inner')
df = pd.merge(train,tdf, on =['date', 'product_identifier','outlet'], how='inner')
df.head(6)

"""


---



---


"""

df.shape

#lets check for Null Values
df.isnull().sum()

#check the data types
df.dtypes

#Lets check which columns contains '?'
df[df.columns[(df == '?').any()]].nunique()

#Lets chcek the value counts for categorical data
for i in df.columns:
    if df[i].dtypes == 'object':
        print(df[i].value_counts())
        print('-----------------------------------')

"""---

---

# **Data processing**
"""

df['date'] = pd.to_datetime(df['date'])

df['date_year'] = df['date'].dt.year
df['date_mont'] = df['date'].dt.month
#df['date_day'] = df['date'].dt.date

df.drop(['date'], axis=1, inplace =True)
df.head(4)

df['date_mont'].nunique()

df.info()

"""

---



---

"""

df.describe().T

"""

---



---

"""

#df_new1= pd.get_dummies(df, columns=['state','category_of_product','outlet','department_identifier'],drop_first=True)

"""

---



---

"""

#df_new1.head(5)

#df_new1.info()

df.shape

"""

---



---



---

"""

display(df.drop_duplicates())

"""---



---



---

## **Split Data into x & y**

---




---



---
"""

#lets saperate data into label and features
x = df.drop(columns = 'sales')
y = df["sales"]

x.skew()

"""

---



---

"""

x['sell_price'] = np.log(x['sell_price'])
#x['category_of_product_others'] = np.log(x['category_of_product_others'])
#x['department_identifier_12'] = np.log(x['department_identifier_12'])

x.skew()

sns.set_theme()
plt.figure(figsize = (20,8))
plt.subplot(1,2,1)
sns.distplot(x.sell_price)
plt.title('Distriution for sell_price')

plt.show()

"""

---



---

"""

cat_data  = x.select_dtypes(include = 'object')
num_data = x.select_dtypes(include = [np.number])
from sklearn.preprocessing import OrdinalEncoder
enc = OrdinalEncoder()
for i in cat_data.columns:
    cat_data[i] = enc.fit_transform(cat_data[i].values.reshape(-1,1))

cat_data

num_data = x.select_dtypes(include = [np.number])

num_data.head(4)

num_data.info()

num = num_data.reset_index(drop=True)
cat_data = cat_data.reset_index(drop=True)

"""## **Combine Catagorical and Numerical **"""

X = pd.concat([num, cat_data], axis = 1)

X.head()

X.shape

"""---



---



---

# Applying standard scaler to numerical data

---



---
"""

#num_data.replace([np.inf, -np.inf], np.nan, inplace=True)

#num_data.fillna(0, inplace=True)

#Lets bring all numerical features to common scale by applying standard scaler
scaler = StandardScaler()
num = scaler.fit_transform(num_data)
num = pd.DataFrame(num,columns=num_data.columns)

num

"""

---



---

"""

X = num_data

#X.drop(['week_id'], axis=1, inplace =True)

#X.drop(['sell_price'], axis=1, inplace =True )

X.shape

X

"""

---



---



---

"""

X.info()

X.isna().sum().sum()

"""---



---



---

## **Finding best random_state**
"""

#to find random stat which gives maximum r2_score

from sklearn.metrics import r2_score
from sklearn.linear_model import LinearRegression
max_r_score=0
r_state = 0
for i in range(50,1000):
    x_train, x_test, y_train, y_test = train_test_split(X, y,test_size = 0.25,random_state = r_state)
    reg = LinearRegression()
    reg.fit(x_train,y_train)
    y_pred = reg.predict(x_test)
    r2_scr=r2_score(y_test,y_pred)
    if r2_scr > max_r_score:
        max_r_score = r2_scr
        r_state = i
print("max r2 score is",max_r_score,"on Random State",r_state)

#lets split our train data into train and test part with our best random state
x_train, x_test, y_train, y_test = train_test_split(X, y,test_size = 0.25,random_state = 52)

"""---



---



---

## **Building a function for model with evaluation**
"""

from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score

def BuiltModel(model):
    model.fit(x_train,y_train)
    y_pred = model.predict(x_train)
    pred = model.predict(x_test)

    r2score = r2_score(y_test,pred)*100

    #evaluation
    mse = mean_squared_error(y_test,pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test,pred)
    print("MAE :", mae)
    print("RMSE :", rmse)
    print('------------------------------')

    # r2 score
    print("Training r2 Score in percentage :", r2_score(y_train,y_pred)*100,'%')
    print(f"Testing r2 Score in percentage :", r2score,"%")
    print('------------------------------')

    #cross validation score
    scores = cross_val_score(model, X, y, cv = 10).mean()*100
    print("\nCross validation score :", scores)

    #result of accuracy minus cv score
    result = r2score - scores
    print("\nAccuracy Score - Cross Validation Score :", result)

    sns.regplot(y_test,pred)
    plt.show()

"""---



---



---

## **LinearRegression Model**

---



---
"""

from sklearn.linear_model import LinearRegression

lr = LinearRegression()
BuiltModel(lr)

"""---



---



---

## **DecisionTreeRegressor Model**

---



---
"""

from sklearn.tree import DecisionTreeRegressor
dt = DecisionTreeRegressor()
BuiltModel(dt)

"""---



---

## **RandomForestRegressor Model**

---



---
"""

#model with RandomForestClassifier
from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor()
BuiltModel(rf)

"""---



---



---

## **XGBRegressor model**

---



---
"""

from xgboost import XGBRegressor
xgb = XGBRegressor(verbosity = 0)
BuiltModel(xgb)

"""---



---

## **ExtraTreesRegressor model**

---
"""

from sklearn.ensemble import ExtraTreesRegressor
ext = ExtraTreesRegressor()
BuiltModel(ext)

"""## **LGBMRegressor model**"""

from lightgbm import LGBMRegressor
gbm = LGBMRegressor()
BuiltModel(gbm)

"""---



---



---

## **HyperParameter Tuning**

---



---
"""

#lets selects different parameters for tuning
grid_params = {
                'max_depth': [8,9,10,12],
                'n_estimators':[500,700,900,1000],
                'min_samples_split': [2]
                }

#train the model with given parameters using GridSearchCV
#GCV =  GridSearchCV(RandomForestRegressor(), grid_params, cv = 5)
#GCV.fit(x_train,y_train)

#GCV.best_params_       #printing the best parameters found by GridSearchCV

"""---



---

## **Final Model**

---



---
"""

#lets train and test our final model with best parameters
model = LGBMRegressor(max_depth = 12,min_samples_split = 2, n_estimators = 700)
model.fit(x_train,y_train)
pred = model.predict(x_test)

r2score = r2_score(y_test,pred)*100

#evaluation
mse = mean_squared_error(y_test,pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test,pred)
print("MAE :", mae)
print("RMSE :", rmse)
print('------------------------------')

# r2 score

print(f" \nr2 Score:", r2score,"%")

"""---



---

## **Lets see final Actual Vs Predicted sample**
"""

data = pd.DataFrame({'Y Test':y_test , 'Pred':pred},columns=['Y Test','Pred'])
sns.lmplot(x='Y Test',y='Pred',data=data,palette='rainbow')
plt.show()

"""---



---

## **Model Saving**
"""

import joblib
joblib.dump(model,"122_hackaton.pkl")

model = joblib.load('122_hackaton.pkl')

"""---



---

# ***Making predictions for test dataset using final model***

---



---
"""

#week2_id = pd.read_csv('/content/date_to_week_id_map.csv')
#week2_id.head(2)

product2_price = pd.read_csv('/content/product_prices.csv')
product2_price.head(3)

test = pd.read_csv('/content/test_data.csv')
test.head(5)

#tdf2 = pd.merge(product2_price, week2_id, on=['week_id'], how='inner')

df_test = pd.merge(test,product2_price, on =['product_identifier','outlet'], how='inner')
df_test.head(6)







"""

---



---

"""

df_test.shape

p_ID = df_test['id']
df_test = df_test.drop(columns='id')

df_test.shape

df_test.reset_index(inplace=True)

df_test.info()

df_test.isna().sum().sum()

df_test['date'] = pd.to_datetime(df_test['date'])

df_test['date_year'] = df_test['date'].dt.year
df_test['date_mont'] = df_test['date'].dt.month
#df_test['date_day'] = df_test['date'].dt.date

df_test.drop(['date'], axis=1, inplace =True)
df_test.head(4)

#df_new1= pd.get_dummies(df_test, columns=['state','category_of_product'],drop_first=True)

display(df_test.drop_duplicates())

#####################################################################################

#lets saperate data into label and features
#x1 = df_test.drop(columns = 'sales')
#y1 = df_test["sales"]

x1 =df_test

x1.skew()

x1['sell_price'] = np.log(x1['sell_price'])
x1['product_identifier'] = np.log(x1['product_identifier'])
x1['department_identifier'] = np.log(x1['department_identifier'])
x1['product_identifier'] = np.log(x1['product_identifier'])

x1.skew()

catt_data  = x1.select_dtypes(include = 'object')
numm_data = x1.select_dtypes(include = [np.number])
from sklearn.preprocessing import OrdinalEncoder
enc = OrdinalEncoder()
for i in cat_data.columns:
    catt_data[i] = enc.fit_transform(catt_data[i].values.reshape(-1,1))

catt_data.head(6)

numm_data.head(6)

numm_data = x1.select_dtypes(include = [np.number])



#Lets bring all numerical features to common scale by applying standard scaler
scaler = StandardScaler()
numm = scaler.fit_transform(numm_data)
numm = pd.DataFrame(numm,columns=numm_data.columns)

numm.shape

X.shape

X.columns

numm.columns

numm.drop(['index'], axis=1, inplace =True )

numm.head(9)

"""

---



---

"""

#lets predict the price with our best model
prediction = model.predict(numm)

prediction

#lets make the dataframe for prediction
sales_value = pd.DataFrame(prediction, columns=["sale"])

"""

---



---

"""

sub_file = pd.concat([p_ID, sales_value], axis = 1)

sub_file

"""

---




"""

#Lets save the submission to csv
sub_file.to_csv("Srivathsa_N_T_119_MITH.csv",index=False)

"""

---



---


"""

###############################################################

"""# ***THANK YOU*** 

"""

################################################################

"""

---



---

"""